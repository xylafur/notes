We were talking about AVL trees, and discussed single left rotation

We will continue talking about AVL trees and firstly will discuss double left 
rotation

First we assume we have a balanced AVL tree
If we insert the new element and it lands on the left side of the right side,
    (where the star is)

                N
              /   \
            L      R
                  / \
                LR   RR
                *

    More specifically, if it is added at either star
        
                LR
              /    \
            RLL     RLR
             *       *

    This will require a double left rotation

                N
              /   \
            L      R
                  / \
                LR   RR
              /   \
            RLL   RLR
             *     *

                LR
              /    \
            N       R
          /   \   /   \
        L    RLL RLR   RR
              *   *

    We have to verify that this is a search tree and that it is still height
    balanced

    The subtrees other than RLL or RLR will not have been altered, meaning they
    will retain their avl nature

    We know that the AVL nature could not be fucked up by more than 2 (since we
    already had an AVL tree)

Noodes and Inserting:
    each node has info and height information about the tree that is rooted in 
    that node

    Leafs have a height of 0
    After we insert we will have to update the height of each node that we
    traversed through

    While updatig, we can locate violations
        if we find a violation, we rebalance.  This will be the only rebalncing
        opeeration we will have to do because the overall tree has the same 
        height after the inserting and rebalancing

    WHile going back up, if we find a point where we don't have to change the
    height, we won't have to rebalance

It should be clear that we need at most one balancing operation

Complexity:
    
    find
        time:
            O(logn) becaue of the height of the tree

        Space:
            O(1) because it can be done iterativey

    insert
        time:
            no more than 5 pointers to chase
            which means that the rotation is O(1)

            so overall O(logn)

        Space:
            O(logn) because we need information to backtrack (height number of
                    nodes)

    Deletion:
        (Nothing much has changed form the statement made about the standard
         search tree)

         deleting leafs are no issue.. syke we can screw up the height conditions

         But what happens is that when we do that it follows that we again have 
         to go back up along that path.  SO now instead of inserting we delete 
         that element, that means that we will have to update all elements above 
         it.  SO we go up and we have to test again the height condition.  
         We have to determine what is the height of the left sub tree, what is 
         the height of the right subtree (on that path).  As long as they are 
         in complience with that height condition, that is as long as they 
         differ by no more than 1, they are fine.  The moment that we have a 
         difference that is greater than 1, it is obviousy that that can only 
         be 2, at that point we have to do a rebalancing operation.  Now going 
         the other way

         The only problem with that is that in contrast to the insertion, we 
         can actually be screwed in every step.  We may have to rebalance, not 
         just once, but almost every step up.  

         TO see that, lets take a minimum AVL tree

         Still only O(logn) (because the rebasing is always constant)

         Time:
            O(logn)

         Space:
            O(logn)

        Now there are some costs, the costs are that in additon to having the 
        information you also need the pointers and the height information.  
        That is inheritley required to construct n AVL tree.  So it is quite 
        possible that the information that you store is, (assuming we need a 
        byte for the height and at least a byte for each pointers, so a minimum 
        of 3 bytes that are just part of the data structire).  But this is an 
        on-line algorithm, because clearly you acn get elements and insert them, 
        delete them and find them.  So all of these are typically operations 
        that you would carry out in an on-line algorithm.


Huffman codes:
    Optimal huffman code: basically a way of representing information using 
    information about the symols that are used to represent the information

    FOr instance, if we are looking at an english text, the standard representation 
    would be using ascii, 8 bits for each character.  
    That is a convenient representation but it is wasteful because you have a 
    good bit of information about the probablility distribution about the characters

    Certain letters occur far more often than other characters.

    As a result, you are better off, using varaible length codes.  
    What you want to do is use a short code word for something that occurs often
    and a longer code word for someting that appears less frequently.  So you get
    pubishhed for alonger code word (but that doesnt happen often) but ge rewarded
    for having a short code word (which happens often)


    If you have these symbols with these probablilities of occuring:
       
        Symbol  Probablity      Representation
        a1      1/2             1
        a2      1/(2^2)         01
        a3      1/(2^3)         001
        a4      1/(2^4)         0001


    The sum of all of these probablilities is

        n
        E  1/(2^i) * i + 1/(n-1)*n
       i=1

       SO what we get here is 2 - (n + 1) / 2^(n-1) + n/2^(n-1) = 2 - 1/2^(n-1)

       THe average code length is less than 2 in this case

    If you did this with a fixed length code, you would need ceil(logn) bits
    For this varaible length you need only 2 bits

    How do we work an optimal huffman code?
        Well if we take an example, 

        suppose we have 9 symbols

            Symbol      Frequency-Count         Code
            a           1                       100010
            b           3                       10000
            c           6                       10010
            d           29                      00
            e           2                       10011
            f           18                      01
            g           15                      101
            h           4                       10011
            i           40                      11

        You find the 2 smalest probability counts and combine them into one
        with the probability (or frequencyy count) that is the sum of the two

        We merge everything into a single symbol with probability 1.  Then we 
        have to  undo that by providing 0 for 1 branch and 1 for the other branch.  
        After we have merged everything into one symbol, we can then differentiate 
        in a fassion similar to how search trees are resolved.


        0                                                                   1
                                        N
                                    /       \
                                  N            N
                               /    \        /    \
                             d        f     N       i
                                         /     \
                                       N         g 
                                    /     \
        
        We can then figre out the average length by ttaking the sum of
            code bit length  * frequency 
        for each of the characters and then dividing by
            the total frequency count


From the homework

        S       P       C               n
    H(symbols, prob, code_words, number_elements_in_symbols):
        find the two symbols a and b with probabilities pa, pb
        merge a and b into a single symbol  with probablility pa + pb
        S' = (S - {a, b}) v {x}
        H(S', P', C', n-1)
        let w be the code word for x, then the code words for a is ow and for b is w
        change c accordingly

    The complexity is basically proportional to the height of the tree, which  is
    of course height of n (because if n is one that is the termination condition)

    But the crutial condition is that we have to find the two symbols with lowest
    probablility.  If we do that in the straightforward way, it will require us to
    do a linear amount of work.  (we could kep S always sorted)(just kidding sorting is worse)

    We can stick them into an avl tree, find the smallest and the one before that.
    Pop them out, then add the merged back in.  That takes O(logn)

    The optimal timing will be O(nlogn)
