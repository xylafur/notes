Logarithmic Search
    If you have a (very good) Probability distribution, you can achieve time
    complexity of O(log(log(n)))

    Though if your probability distribution isn't good, or no knowledge of the
    dataset then binary search is optimal.

    However if you have knowledge about your dataset, then logarithmic search is
    optimal

Hashing:
    how hashing would work if you wanted to do it in the real world:
        you have a stack of papers, you want to be able to get any particular
        paper in the most efficnet way possible

        Take the stack of papers and throw it in the air! 

        Look at where each paper lands

        Now you know where each paper is

    Ultimately a randomization

    We have a keyspace and a hashtable

    Collisions:
        Multiple distinct keys may map to the same value

        If you have a collision, you have a set of addresses, you check each 
        element from that set

    For each key, we have a sequence of randomly distributed addresses that have
    only one property: collectively they cover the entire space of the hash table.
    The motivation behind this is if there is any space in the hash table that
    could be used, we want to use it.


    Where will this all lead?
        We talked about logarithmic search, where the idea was we have an idea
        where that thing we are looking for should be.  In theory, if we knew
        exactly where the element is, we need only one probe.  This is basically
        the idea of hashing.  If done right, you need 2 probes to find a alue
